## The Code

The code that we will use for this project first reads in the outputted csv from MTurk. It then filters out the pre-annotated tasks that we included for quality control. Based on these pre-annotated tasks, we use the get_worker_scores() function to generate average worker similarity scores. This uses the get_similarity() function, which compares the percentage similarity between the workerâ€™s annotation text and the pre-annotated text. Once the average worker scores are calculated, we call the get_aggregated_results() function to get the final aggregated annotations for each task. This is done by sorting the workers based on their similarity score, and then taking the annotations of the worker with the highest average similarity score for the quality control tasks. Finally, these aggregated annotations are written to a csv file with the original transcription of the text and the chosen aggregated translation.

## Our Planned Analysis

We are planning on doing analysis focusing on the quality of translations done by our workers. For example, it would be interesting to see which group, if any, of the workers are particularly better at producing higher quality translations: Are native speakers better at translating than non-native speakers? Related to this would be analyzing the location of workers to see if this affects translations. This could be applicable for a language that is spoken over a large amount of countries, like Spanish. Perhaps, Latin American workers will translate a Spanish nursery rhyme differently from workers in Spain. We could also analyze how accurate the Google Translate version is by measuring how many changes are made by workers to that translation. For both possible analyses we are planning on using some text comparison function to determine differences between various translations submitted by workers. 
